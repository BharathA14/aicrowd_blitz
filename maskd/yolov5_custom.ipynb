{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yolov5_custom.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "#temporary until bug is fixed in current master\n",
        "!git reset --hard 5ba1de0cdcc414c69ceb7a4c45eb1e3895eca32a\n",
        "%cd /content\n",
        "\n",
        "!pip install -r yolov5/requirements.txt  # install dependencies\n",
        "%cd yolov5\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knxi2ncxWffW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DATASET\n",
        "\n",
        "%cd /content\n",
        "!curl -L \"https://app.roboflow.ai/ds/mMG2vNixeM?key=xdY9j3bE0x\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BcAUTAdFQhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TEST IMAGES FROM AICROWD\n",
        "\n",
        "%cd /content/test/\n",
        "# !ls\n",
        "!wget \"https://s3.eu-central-1.wasabisys.com/aicrowd-practice-challenges/public/maskd/v0.1/test_images.zip\"\n",
        "!unzip test_images.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ3DmmGQztJj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "48a5deb8-11f5-4bfa-8508-bf03ad74ccb1"
      },
      "source": [
        "%cat data.yaml"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat: data.yaml: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwJx-2NHsYxT",
        "colab_type": "text"
      },
      "source": [
        "# Define Model Configuration and Architecture\n",
        "\n",
        "We will write a yaml script that defines the parameters for our model like the number of classes, anchors, and each layer.\n",
        "\n",
        "You do not need to edit these cells, but you may."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOPn9wjOAwwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define number of classes based on YAML\n",
        "import yaml\n",
        "with open(\"data.yaml\", 'r') as stream:\n",
        "    num_classes = str(yaml.safe_load(stream)['nc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDxebz13RdRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/\n",
        "##write custom model .yaml\n",
        "#you can configure this based on other YOLOv5 models in the models directory\n",
        "with open('yolov5/models/custom_yolov5s.yaml', 'w') as f:\n",
        "  # parameters\n",
        "  f.write('nc: ' + num_classes + '\\n')\n",
        "  #f.write('nc: ' + str(len(class_labels)) + '\\n')\n",
        "  f.write('depth_multiple: 0.33'  + '\\n') # model depth multiple\n",
        "  f.write('width_multiple: 0.50'  + '\\n')  # layer channel multiple\n",
        "  f.write('\\n')\n",
        "  f.write('anchors:' + '\\n')\n",
        "  f.write('  - [10,13, 16,30, 33,23] ' + '\\n')\n",
        "  f.write('  - [30,61, 62,45, 59,119]' + '\\n')\n",
        "  f.write('  - [116,90, 156,198, 373,326] ' + '\\n')\n",
        "  f.write('\\n')\n",
        " \n",
        "  f.write('backbone:' + '\\n')\n",
        "  f.write('  [[-1, 1, Focus, [64, 3]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [128, 3, 2]],' + '\\n')\n",
        "  f.write('   [-1, 3, Bottleneck, [128]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [256, 3, 2]],' + '\\n')\n",
        "  f.write('   [-1, 9, BottleneckCSP, [256]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [512, 3, 2]], ' + '\\n')\n",
        "  f.write('   [-1, 9, BottleneckCSP, [512]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [1024, 3, 2]],' + '\\n')\n",
        "  f.write('   [-1, 1, SPP, [1024, [5, 9, 13]]],' + '\\n')\n",
        "  f.write('   [-1, 6, BottleneckCSP, [1024]],' + '\\n')\n",
        "  f.write('  ]' + '\\n')\n",
        "  f.write('\\n')\n",
        " \n",
        "  f.write('head:'  + '\\n')\n",
        "  f.write('  [[-1, 3, BottleneckCSP, [1024, False]],'  + '\\n')\n",
        "  f.write('   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1, 0]],' + '\\n')\n",
        "  f.write('   [-2, 1, nn.Upsample, [None, 2, \"nearest\"]],' + '\\n')\n",
        "  \n",
        "  f.write('   [[-1, 6], 1, Concat, [1]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [512, 1, 1]],' + '\\n')\n",
        "  f.write('   [-1, 3, BottleneckCSP, [512, False]],' + '\\n')\n",
        "  f.write('   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1, 0]],' + '\\n')\n",
        "  \n",
        "  f.write('   [-2, 1, nn.Upsample, [None, 2, \"nearest\"]],' + '\\n')\n",
        "  f.write('   [[-1, 4], 1, Concat, [1]],' + '\\n')\n",
        "  f.write('   [-1, 1, Conv, [256, 1, 1]],' + '\\n')\n",
        "  f.write('   [-1, 3, BottleneckCSP, [256, False]],' + '\\n')\n",
        "  f.write('   [-1, 1, nn.Conv2d, [na * (nc + 5), 1, 1, 0]],' + '\\n')\n",
        "  f.write('\\n' )\n",
        "  f.write('   [[], 1, Detect, [nc, anchors]],' + '\\n')\n",
        "  f.write('  ]' + '\\n')\n",
        " \n",
        "print('custom model config written!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcFxRcFdJ_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "%%time \n",
        "%cd /content/yolov5/\n",
        "# from IPython.utils import io\n",
        "#lr 0.001\n",
        "# with io.capture_output() as captured:\n",
        "!python train.py --img 416 --batch 16 --epochs 600 --data '../data.yaml' --cfg ./models/custom_yolov5s.yaml --weights ' ' --name yolov5s_results --adam "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOy5KI2ncnWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start tensorboard\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jal-Ov5Wer5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################################\n",
        "'''\n",
        "replace detect.py with the following code\n",
        "'''\n",
        "\n",
        "\n",
        "import argparse\n",
        "\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from utils import google_utils\n",
        "from utils.datasets import *\n",
        "from utils.utils import *\n",
        "\n",
        "\n",
        "def detect(save_img=False):\n",
        "    out, source, weights, view_img, save_txt, imgsz = \\\n",
        "        opt.output, opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size\n",
        "    webcam = source == '0' or source.startswith('rtsp') or source.startswith('http') or source.endswith('.txt')\n",
        "\n",
        "    # Initialize\n",
        "    device = torch_utils.select_device(opt.device)\n",
        "    if os.path.exists(out):\n",
        "        shutil.rmtree(out)  # delete output folder\n",
        "    os.makedirs(out)  # make new output folder\n",
        "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
        "\n",
        "    # Load model\n",
        "    google_utils.attempt_download(weights)\n",
        "    model = torch.load(weights, map_location=device)['model'].float()  # load to FP32\n",
        "    # torch.save(torch.load(weights, map_location=device), weights)  # update model if SourceChangeWarning\n",
        "    # model.fuse()\n",
        "    model.to(device).eval()\n",
        "    imgsz = check_img_size(imgsz, s=model.model[-1].stride.max())  # check img_size\n",
        "    if half:\n",
        "        model.half()  # to FP16\n",
        "\n",
        "    # Second-stage classifier\n",
        "    classify = False\n",
        "    if classify:\n",
        "        modelc = torch_utils.load_classifier(name='resnet101', n=2)  # initialize\n",
        "        modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model'])  # load weights\n",
        "        modelc.to(device).eval()\n",
        "\n",
        "    # Set Dataloader\n",
        "    vid_path, vid_writer = None, None\n",
        "    if webcam:\n",
        "        view_img = True\n",
        "        cudnn.benchmark = True  # set True to speed up constant image size inference\n",
        "        dataset = LoadStreams(source, img_size=imgsz)\n",
        "    else:\n",
        "        save_img = True\n",
        "        dataset = LoadImages(source, img_size=imgsz)\n",
        "\n",
        "    # Get names and colors\n",
        "    names = model.module.names if hasattr(model, 'module') else model.names\n",
        "    colors = [[random.randint(0, 255) for _ in range(3)] for _ in range(len(names))]\n",
        "\n",
        "    # Run inference\n",
        "    t0 = time.time()\n",
        "    img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
        "    _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n",
        "    for path, img, im0s, vid_cap in dataset:\n",
        "        img = torch.from_numpy(img).to(device)\n",
        "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "        if img.ndimension() == 3:\n",
        "            img = img.unsqueeze(0)\n",
        "\n",
        "        # Inference\n",
        "        t1 = torch_utils.time_synchronized()\n",
        "        pred = model(img, augment=opt.augment)[0]\n",
        "\n",
        "        # Apply NMS\n",
        "        pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
        "        t2 = torch_utils.time_synchronized()\n",
        "\n",
        "        # Apply Classifier\n",
        "        if classify:\n",
        "            pred = apply_classifier(pred, modelc, img, im0s)\n",
        "\n",
        "        # Process detections\n",
        "        for i, det in enumerate(pred):  # detections per image\n",
        "            if webcam:  # batch_size >= 1\n",
        "                p, s, im0 = path[i], '%g: ' % i, im0s[i].copy()\n",
        "            else:\n",
        "                p, s, im0 = path, '', im0s\n",
        "\n",
        "            save_path = str(Path(out) / Path(p).name)\n",
        "            txt_path = str(Path(out) / Path(p).stem) + ('_%g' % dataset.frame if dataset.mode == 'video' else '')\n",
        "            s += '%gx%g ' % img.shape[2:]  # print string\n",
        "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "            if det is not None and len(det):\n",
        "                # Rescale boxes from img_size to im0 size\n",
        "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
        "\n",
        "                # Print results\n",
        "                for c in det[:, -1].unique():\n",
        "                    n = (det[:, -1] == c).sum()  # detections per class\n",
        "                    s += '%g %ss, ' % (n, names[int(c)])  # add to string\n",
        "                '''''\n",
        "                      Modifications done:\n",
        "                      Initially the code was set to print bb based on normalized values. Changed it to actual coordinates\n",
        "                      Confident score included in the txt file\n",
        "                      Class added\n",
        "                '''''\n",
        "                # Write results\n",
        "                for *xyxy, conf, cls in det:\n",
        "                    if True:  # Write to file\n",
        "                        xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "                        with open(txt_path + '.txt', 'a') as f:\n",
        "                            f.write(('%g %f %d %d %d %d'+ '\\n') % (cls+1, conf, int(xyxy[0]), int(xyxy[1]), int(xyxy[2])- int(xyxy[0]), int(xyxy[3])-int(xyxy[1])))  # cls +1 since i mapped the classes incorrectly\n",
        "\n",
        "                    if save_img or view_img:  # Add bbox to image\n",
        "                        label = '%s %.2f' % (names[int(cls)], conf)\n",
        "                        plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=3)\n",
        "\n",
        "            # Print time (inference + NMS)\n",
        "            print('%sDone. (%.3fs)' % (s, t2 - t1))\n",
        "\n",
        "            # Stream results\n",
        "            if view_img:\n",
        "                cv2.imshow(p, im0)\n",
        "                if cv2.waitKey(1) == ord('q'):  # q to quit\n",
        "                    raise StopIteration\n",
        "\n",
        "            # Save results (image with detections)\n",
        "            if save_img:\n",
        "                if dataset.mode == 'images':\n",
        "                    cv2.imwrite(save_path, im0)\n",
        "                else:\n",
        "                    if vid_path != save_path:  # new video\n",
        "                        vid_path = save_path\n",
        "                        if isinstance(vid_writer, cv2.VideoWriter):\n",
        "                            vid_writer.release()  # release previous video writer\n",
        "\n",
        "                        fourcc = 'mp4v'  # output video codec\n",
        "                        fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
        "                        w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "                        h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "                        vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*fourcc), fps, (w, h))\n",
        "                    vid_writer.write(im0)\n",
        "\n",
        "    if save_txt or save_img:\n",
        "        print('Results saved to %s' % os.getcwd() + os.sep + out)\n",
        "        if platform == 'darwin':  # MacOS\n",
        "            os.system('open ' + save_path)\n",
        "\n",
        "    print('Done. (%.3fs)' % (time.time() - t0))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--weights', type=str, default='weights/yolov5s.pt', help='model.pt path')\n",
        "    parser.add_argument('--source', type=str, default='inference/images', help='source')  # file/folder, 0 for webcam\n",
        "    parser.add_argument('--output', type=str, default='inference/output', help='output folder')  # output folder\n",
        "    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n",
        "    parser.add_argument('--conf-thres', type=float, default=0.4, help='object confidence threshold')\n",
        "    parser.add_argument('--iou-thres', type=float, default=0.5, help='IOU threshold for NMS')\n",
        "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--view-img', action='store_true', help='display results')\n",
        "    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
        "    parser.add_argument('--classes', nargs='+', type=int, help='filter by class')\n",
        "    parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
        "    parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
        "    parser.add_argument('--update', action='store_true', help='update all models')\n",
        "    opt = parser.parse_args()\n",
        "    print(opt)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if opt.update:  # update all models (to fix SourceChangeWarning)\n",
        "            for opt.weights in ['yolov5s.pt', 'yolov5m.pt', 'yolov5l.pt', 'yolov5x.pt', 'yolov3-spp.pt']:\n",
        "                detect()\n",
        "                create_pretrained(opt.weights, opt.weights)\n",
        "        else:\n",
        "            detect()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nmZZnWOgJ2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# when we ran this, we saw .007 second inference time. That is 140 FPS on a TESLA P100!\n",
        "%cd /content/yolov5/\n",
        "# !mkdir output\n",
        "!python detect.py --weights '/content/yolov5/weights/last_yolov5x_results.pt' --img 416 --conf 0.4 --source ../test/test_images/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odKEqYtTgbRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#display inference on ALL test images\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for imageName in glob.glob('/content/yolov5/inference/output/*.jpg'): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvBTYcdyzc5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip -r /content/yolo5s_results.zip /content/yolov5/inference/output/*.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}