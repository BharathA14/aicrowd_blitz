# -*- coding: utf-8 -*-
"""SCRBL- bi-lstm with attn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q3h97F_jPJ-Ais6-LDKdHhtmsOe9Uewy

## Importing data
"""

import tensorflow as tf
tf.test.gpu_device_name()

!rm -rf data
!mkdir data
!wget https://s3.eu-central-1.wasabisys.com/aicrowd-practice-challenges/public/scrbl/v0.1/train.zip
!wget https://s3.eu-central-1.wasabisys.com/aicrowd-practice-challenges/public/scrbl/v0.1/val.zip
!wget https://s3.eu-central-1.wasabisys.com/aicrowd-practice-challenges/public/scrbl/v0.1/test.zip
!unzip train.zip
!unzip val.zip
!unzip test.zip
!mv train.csv data/train.csv
!mv val.csv data/val.csv
!mv test.csv data/test.csv

import pandas as pd
import numpy as np

train = pd.read_csv('data/train.csv')
val = pd.read_csv('data/val.csv')
test = pd.read_csv('data/test.csv')

train.head()

print('train:',train.shape,'\nval:',val.shape,'\ntest:',test.shape)

"""## Cleaning"""

import nltk
nltk.download('stopwords')
nltk.download('wordnet')
  
import re
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
	text = re.sub(r'[^\w\s]','',text, re.UNICODE)
	text = text.lower()
	text = [lemmatizer.lemmatize(token) for token in text.split(" ")]
	text = [lemmatizer.lemmatize(token, "v") for token in text]
	text = [word for word in text if  not word in stop_words]
	text = " ".join(text)
	return text

train['cleaned_text'] = train.text.apply(lambda x: clean_text(x))
val['cleaned_text'] = val.text.apply(lambda x: clean_text(x))
test['cleaned_text'] = test.text.apply(lambda x: clean_text(x))

from sklearn.preprocessing import LabelEncoder

lb = LabelEncoder()
ytrain = lb.fit_transform(train.label.values)
yvalid = lb.fit_transform(val.label.values)

xtrain = train.cleaned_text
xvalid = val.cleaned_text
xtest = test.cleaned_text

print (xtrain.shape)
print (xvalid.shape)

xtrain

"""## Glove Embedding"""

from google.colab import drive
drive.mount('/gdrive')

dir_name = "/gdrive/My Drive/glove/"

f = open(dir_name+'glove.6B.300d.txt')

from tqdm import tqdm
embeddings_index = {}
for line in tqdm(f):
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

print('Found %s word vectors' % len(embeddings_index))

import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords

nltk.download('stopwords')
nltk.download('punkt')

stop_words = stopwords.words('english')

def sent2vec(s):
    words = str(s).lower()
    words = word_tokenize(words)
    words = [w for w in words if not w in stop_words]
    words = [w for w in words if w.isalpha()]
    M = []
    for w in words:
        try:
            M.append(embeddings_index[w])
        except:
            continue
    M = np.array(M)
    v = M.sum(axis=0)
    if type(v) != np.ndarray:
        return np.zeros(300)
    return v / np.sqrt((v ** 2).sum())

xtrain_glove = [sent2vec(x) for x in xtrain]
xvalid_glove = [sent2vec(x) for x in xvalid]

xtest_glove = [sent2vec(x) for x in xtest]

from sklearn.preprocessing import StandardScaler

scl = StandardScaler()
xtrain_glove_scl = scl.fit_transform(xtrain_glove)
xvalid_glove_scl = scl.transform(xvalid_glove)

xtest_glove_scl = scl.transform(xtest_glove)

xvalid_glove_scl.shape

from keras.utils import np_utils

ytrain_enc = np_utils.to_categorical(ytrain)
yvalid_enc = np_utils.to_categorical(yvalid)

"""## Model"""

import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Concatenate, Dense, Input, LSTM, Embedding, Dropout, Activation
from tensorflow.keras.layers import Bidirectional
from tensorflow.keras.models import Model, Sequential

class Attention(tf.keras.Model):
	def __init__(self, units):
		super(Attention, self).__init__()
		self.W1 = tf.keras.layers.Dense(units)
		self.W2 = tf.keras.layers.Dense(units)
		self.V = tf.keras.layers.Dense(1)

	def call(self, features, hidden):
		hidden_with_time_axis = tf.expand_dims(hidden, 1)
		score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))
		attention_weights = tf.nn.softmax(self.V(score), axis=1)
		context_vector = attention_weights * features
		context_vector = tf.reduce_sum(context_vector, axis=1)
		return context_vector, attention_weights

train.cleaned_text.apply(lambda x: len(x.split(" "))).mean()

from keras.preprocessing import sequence, text

token = text.Tokenizer(num_words=None)
max_len = 32

token.fit_on_texts(list(xtrain) + list(xvalid))
xtrain_seq = token.texts_to_sequences(xtrain)
xvalid_seq = token.texts_to_sequences(xvalid)

# zero padding
xtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)
xvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)

word_index = token.word_index

xtest_seq = token.texts_to_sequences(xtest)

# zero padding
xtest_pad = sequence.pad_sequences(xtest_seq, maxlen=max_len)

# embedding matrix

embedding_matrix = np.zeros((len(word_index)+1, 300))
for word, i in tqdm(word_index.items()):
  embedding_vector = embeddings_index.get(word)
  if embedding_vector is not None:
    embedding_matrix[i] = embedding_vector

xtrain_pad.shape

sequence_input = Input(shape=(max_len,), dtype='int32')

embedded_sequences = Embedding(len(word_index)+1, 300)(sequence_input)

RNN_CELL_SIZE = 32
lstm = Bidirectional(LSTM(RNN_CELL_SIZE, return_sequences = True), name="bi_lstm_0")(embedded_sequences)


(lstm, forward_h, forward_c, backward_h, backward_c) = Bidirectional(LSTM(RNN_CELL_SIZE, return_sequences=True, return_state=True), name="bi_lstm_1")(lstm)

state_h = Concatenate()([forward_h, backward_h])
state_c = Concatenate()([forward_c, backward_c])
context_vector, attention_weights = Attention(10)(lstm, state_h)
dense1 = Dense(20, activation="relu")(context_vector)
dropout = Dropout(0.05)(dense1)
output = Dense(2, activation="sigmoid")(dropout)
  
model = keras.Model(inputs=sequence_input, outputs=output)

model.summary()

METRICS = [keras.metrics.Accuracy(name='accuracy')]

model.compile(
    optimizer=keras.optimizers.RMSprop(learning_rate=0.001),
    loss=keras.losses.CategoricalCrossentropy(),
    metrics=METRICS)

model.fit(xtrain_pad, y=ytrain_enc, batch_size=100, epochs=5, validation_data=(xvalid_pad, yvalid_enc), verbose=1)

y_pred = model.predict_classes(xtest_pad, verbose=1)

y_pred = lb.inverse_transform(y_pred)

y_val = yvalid
y_val = lb.inverse_transform(y_val)

from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score,log_loss

precision = precision_score(y_val,y_pred,average='micro')
recall = recall_score(y_val,y_pred,average='micro')
accuracy = accuracy_score(y_val,y_pred)
f1 = f1_score(y_val,y_pred,average='macro')

print("Accuracy of the model is :" ,accuracy)
print("Recall of the model is :" ,recall)
print("Precision of the model is :" ,precision)
print("F1 score of the model is :" ,f1)

submission = pd.DataFrame(y_pred)
submission.to_csv('submission.csv',header=['label'],index=False)